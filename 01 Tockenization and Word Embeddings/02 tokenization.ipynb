{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c10aee",
   "metadata": {},
   "source": [
    "# Tokenization Preprocessing for BERT-style Models\n",
    "\n",
    "In transformer models such as BERT and DistilBERT, tokenization is only the first part of a larger **input preprocessing pipeline**. After tokenization, several additional structures are created so that the model can correctly interpret the input sequence during training and inference.\n",
    "\n",
    "## From Text to Model Input\n",
    "\n",
    "Consider a short input sentence. The preprocessing pipeline transforms it through several stages so that it can be consumed by the transformer encoder. These stages are deterministic and defined by the model architecture.\n",
    "\n",
    "The overall flow is:\n",
    "\n",
    "raw text → tokens → special tokens → token IDs → padding → attention masks → token type IDs\n",
    "\n",
    "Each step adds structure and constraints that the transformer relies on.\n",
    "\n",
    "## Special Tokens: CLS and SEP\n",
    "\n",
    "Transformer encoders based on BERT introduce special tokens that do not correspond to words in the input text.\n",
    "\n",
    "The **CLS token** is inserted at the beginning of every input sequence. It serves as a global representation of the entire sequence. During training, the hidden state corresponding to this token is commonly used for sequence-level tasks such as classification or regression. Conceptually, CLS signals to the model that a new sequence has started and that a summary representation will be required.\n",
    "\n",
    "The **SEP token** marks boundaries between segments. It is used to separate sentences or to signal the end of a sequence. When a single sentence is used, a SEP token is still added to clearly delimit the input.\n",
    "\n",
    "These tokens are part of the model’s fixed vocabulary and are treated like any other token during embedding and attention.\n",
    "\n",
    "## Token IDs\n",
    "\n",
    "After adding special tokens, each token is mapped to a unique integer from the model’s vocabulary. These integers are called **token IDs**. Token IDs are indices into the embedding matrix and are the actual numerical inputs to the model.\n",
    "\n",
    "At this stage, the input is a sequence of integers of variable length.\n",
    "\n",
    "## Fixed-Length Inputs and Padding\n",
    "\n",
    "Transformer models expect inputs of a fixed length. This is necessary for efficient batching and consistent tensor shapes.\n",
    "\n",
    "If the token sequence is shorter than the chosen maximum length, padding is applied. Padding consists of appending a special value (typically zero) until the sequence reaches the required length. If the sequence is longer than the maximum length, it is truncated.\n",
    "\n",
    "Padding does not represent meaningful information. Its only purpose is to enforce a uniform input shape across different samples.\n",
    "\n",
    "## Attention Masks\n",
    "\n",
    "Because padding tokens carry no semantic meaning, the model must be told to ignore them. This is done using an **attention mask**.\n",
    "\n",
    "The attention mask is a binary sequence with the same length as the input:\n",
    "\n",
    "- A value of 1 indicates a real token that should be attended to.\n",
    "- A value of 0 indicates padding that should be ignored.\n",
    "\n",
    "During self-attention, these masks prevent padded positions from influencing the attention scores.\n",
    "\n",
    "## Token Type IDs (Segment IDs)\n",
    "\n",
    "BERT-style models can process pairs of sentences simultaneously. To distinguish between different segments, **token type IDs** are used.\n",
    "\n",
    "Token type IDs assign an integer label to each token indicating which sentence it belongs to. For example:\n",
    "\n",
    "- Tokens from the first sentence are assigned one label.\n",
    "- Tokens from the second sentence are assigned a different label.\n",
    "\n",
    "When only a single sentence is provided, all tokens receive the same token type ID. Padding positions also receive a default value, since they do not belong to any sentence.\n",
    "\n",
    "Token type IDs allow the model to learn relationships between sentence pairs, such as entailment or similarity.\n",
    "\n",
    "## Single-Sequence Input Structure\n",
    "\n",
    "For a single sentence, the model input consists of:\n",
    "\n",
    "- Token IDs representing tokens and special tokens\n",
    "- Attention masks indicating real tokens versus padding\n",
    "- Token type IDs indicating a single segment\n",
    "\n",
    "Even though token type IDs may seem redundant in this case, they are still generated for architectural consistency.\n",
    "\n",
    "## Two-Sequence Input Structure\n",
    "\n",
    "When two sentences are provided together, the input structure changes slightly:\n",
    "\n",
    "- A separator token marks the boundary between sentences\n",
    "- Token type IDs switch values at the sentence boundary\n",
    "- Attention masks still indicate valid tokens versus padding\n",
    "\n",
    "This allows the model to jointly encode both sentences while still preserving their identity.\n",
    "\n",
    "## Why All Steps Matter\n",
    "\n",
    "Although the model ultimately consumes token IDs, attention masks, and token type IDs, each preprocessing step depends on the previous one. Padding must be defined before masks can be created, and sentence boundaries must be identified before token type IDs can be assigned.\n",
    "\n",
    "These preprocessing steps ensure that the transformer encoder receives well-structured, interpretable inputs and behaves consistently during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9000bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e83e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a92856",
   "metadata": {},
   "source": [
    "### Single Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7a98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(\n",
    "    \"I love mathematics!\",          # Raw input text to be tokenized and encoded\n",
    "\n",
    "    add_special_tokens = True,      # Adds special tokens such as [CLS] at the beginning\n",
    "                                    # and [SEP] at the end of the sequence\n",
    "\n",
    "    max_length = 20,                # Forces the final sequence to have a fixed length\n",
    "                                    # (after tokenization and adding special tokens)\n",
    "\n",
    "    padding = 'max_length',         # Pads the sequence with zeros if it is shorter\n",
    "                                    # than max_length\n",
    "\n",
    "    truncation = True,              # Truncates the sequence if it exceeds max_length\n",
    "\n",
    "    return_token_type_ids = True,   # Returns token type IDs (segment IDs)\n",
    "                                    # Used to distinguish multiple sentences\n",
    "\n",
    "    return_attention_mask = True    # Returns the attention mask\n",
    "                                    # 1 for real tokens, 0 for padding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9367a232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [101, 1045, 2293, 5597, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Token type ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Attention Mask: {inputs['attention_mask']}\")\n",
    "print(f\"Token type ids: {inputs['token_type_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aed8c4",
   "metadata": {},
   "source": [
    "### Two Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c263bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(\n",
    "    \"I love mathematics!\",\n",
    "    \"Linear algebra is at the core of machine learning\",\n",
    "    add_special_tokens = True,\n",
    "    max_length = 20,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_token_type_ids = True,\n",
    "    return_attention_mask = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d34365bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [101, 1045, 2293, 5597, 999, 102, 7399, 11208, 2003, 2012, 1996, 4563, 1997, 3698, 4083, 102, 0, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "Token type ids: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Attention Mask: {inputs['attention_mask']}\")\n",
    "print(f\"Token type ids: {inputs['token_type_ids']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
