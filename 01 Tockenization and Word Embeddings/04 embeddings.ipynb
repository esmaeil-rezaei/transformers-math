{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6596ccb",
   "metadata": {},
   "source": [
    "# Embeddings in Transformer Models\n",
    "\n",
    "Embeddings are a critical component in transformer models such as BERT and DistilBERT. They allow tokenized text to be converted into numerical vectors that capture semantic meaning, enabling the model to process language effectively.\n",
    "\n",
    "## What Are Embeddings?\n",
    "\n",
    "After tokenization, each token is mapped to a **vector of numbers** in a continuous vector space. These vectors are called **embeddings**.\n",
    "\n",
    "- Embeddings allow words or tokens with similar meanings to have similar numerical representations.\n",
    "- They enable the model to understand relationships between tokens, such as similarity, context, or semantic meaning.\n",
    "- Embeddings are learned parameters, trained via backpropagation. Pretrained models like BERT and DistilBERT already have embeddings learned from large corpora, which can then be **fine-tuned** for specific tasks.\n",
    "\n",
    "## Why We Need Embeddings\n",
    "\n",
    "Computers cannot process raw text directly; they require numerical inputs. Embeddings serve as a bridge:\n",
    "\n",
    "- Translate tokens into numbers\n",
    "- Represent semantic meaning\n",
    "- Preserve similarity: similar words are close in vector space\n",
    "- Provide dense, fixed-length representations for each token\n",
    "\n",
    "This is analogous to **transfer learning** in computer vision: we leverage pretrained embeddings instead of training from scratch.\n",
    "\n",
    "## Conceptual Example of Embeddings\n",
    "\n",
    "Imagine a 3-dimensional embedding space (simplified for visualization):\n",
    "\n",
    "- **Dimension 1:** Wings\n",
    "- **Dimension 2:** Engine\n",
    "- **Dimension 3:** Sky\n",
    "\n",
    "Tokens representing different entities can be mapped into this space:\n",
    "\n",
    "| Token      | Wings | Engine | Sky |\n",
    "| ---------- | ----- | ------ | --- |\n",
    "| Bee        | 2     | 0      | 4   |\n",
    "| Eagle      | 3     | 0      | 3   |\n",
    "| Goose      | 3     | 0      | 2   |\n",
    "| Jet        | 1     | 1      | 1   |\n",
    "| Helicopter | 0     | 4      | 2   |\n",
    "| Drone      | 0     | 3      | 3   |\n",
    "| Rocket     | 0     | 2      | 4   |\n",
    "\n",
    "- Tokens with similar properties are **close together** in the embedding space (e.g., bee, eagle, and goose).\n",
    "- Tokens with distinct characteristics are further apart (e.g., jet vs. bee).\n",
    "\n",
    "> Note: Real transformer embeddings have **768 dimensions** (for DistilBERT), not just three. The 3D example is purely illustrative.\n",
    "\n",
    "## How Embeddings Are Used\n",
    "\n",
    "1. **Input tokenization:** Raw text is split into tokens.\n",
    "2. **Token IDs:** Each token is converted into a numerical index from the vocabulary.\n",
    "3. **Embedding layer:** Token IDs are mapped to high-dimensional vectors.\n",
    "4. **Transformer layers:** Embeddings are fed into the modelâ€™s transformer layers for further processing, such as attention and contextual encoding.\n",
    "\n",
    "These embeddings capture **semantic relationships** between tokens, which are crucial for tasks like classification, translation, or question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f773de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff4b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love mathematics!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c481964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'love', 'mathematics', '!']\n",
      "Token IDs: [1045, 2293, 5597, 999]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63cef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([1, 4, 768])\n",
      "Token: i\n",
      "Embedding (first 10 dims): tensor([ 0.0648,  0.3697,  0.0214,  0.0242, -0.2595,  0.0748,  0.3394,  0.0936,\n",
      "        -0.2900, -0.3007])\n",
      "Token: love\n",
      "Embedding (first 10 dims): tensor([ 0.4542,  0.9776,  0.3335, -0.0131, -0.0401, -0.1888,  0.4279, -0.0346,\n",
      "        -0.2702, -0.5418])\n",
      "Token: mathematics\n",
      "Embedding (first 10 dims): tensor([ 0.0373,  1.0036,  0.0657, -0.0548,  0.0196, -0.1038,  0.2634,  0.1846,\n",
      "        -0.4612, -0.2633])\n",
      "Token: !\n",
      "Embedding (first 10 dims): tensor([ 0.1830,  0.9067,  0.3583, -0.0487, -0.2368, -0.2107,  0.6332, -0.0442,\n",
      "        -0.4583, -0.3439])\n"
     ]
    }
   ],
   "source": [
    "# Convert token IDs to tensor and add batch dimension\n",
    "input_ids = torch.tensor([token_ids])\n",
    "\n",
    "# Get embeddings from BERT\n",
    "with torch.no_grad():  # No gradients needed for inference\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs.last_hidden_state  # shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# Print embeddings for each token (first 10 dimensions for brevity)\n",
    "for token, emb in zip(tokens, embeddings[0]):\n",
    "    print(f\"Token: {token}\")\n",
    "    print(f\"Embedding (first 10 dims): {emb[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
