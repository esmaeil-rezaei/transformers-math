{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b76f0f",
   "metadata": {},
   "source": [
    "# Tokenization in Transformers\n",
    "\n",
    "Before a transformer model like BERT, DistilBERT, or GPT can process text, raw text needs to be converted into a format the model understands. This process is called **tokenization**, the first step in transforming human-readable text into numerical data for the model.\n",
    "\n",
    "Tokenization splits text into smaller units called tokens, which can be words, subwords, or characters.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Sentence: `\"Transformers are amazing!\"`\n",
    "\n",
    "- Tokens: `[\"Transformers\", \"are\", \"amazing\", \"!\"]`\n",
    "\n",
    "These tokens are then mapped to token IDs, which are integers the model uses internally:\n",
    "\n",
    "Token IDs: `[1012, 2024, 7894, 999]`\n",
    "\n",
    "## Why Not Just Words?\n",
    "\n",
    "Using only whole words as tokens is inefficient because the vocabulary would need millions of words, and unknown words would be impossible to process. The solution is **subword tokenization**.\n",
    "\n",
    "## Subword Tokenization\n",
    "\n",
    "Subword tokenization breaks words into smaller meaningful pieces.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Word: \"unclear\"\n",
    "\n",
    "- Tokens: [\"un\", \"##clear\"]\n",
    "\n",
    "`\"un\"` is a common prefix indicating negation. `\"##clear\"` is the remainder of the word, with `##` showing continuation from the previous token. Subword tokenization allows the model to handle rare or unseen words, morphological variations, and reduces vocabulary size.\n",
    "\n",
    "## Token IDs\n",
    "\n",
    "After tokenization, each token is converted to a unique integer from the model's vocabulary.\n",
    "\n",
    "Example:\n",
    "\n",
    "- \"are\" → 2003\n",
    "\n",
    "- \"Transformers\" → 7953\n",
    "\n",
    "Token IDs are what the model actually processes.\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "Once token IDs are obtained, they are converted to vectors called embeddings. Embeddings capture semantic meaning and are high-dimensional (e.g., 768 dimensions in BERT).\n",
    "\n",
    "Example:\n",
    "\n",
    "- Token ID: 2003 (\"are\")\n",
    "\n",
    "- Embedding: [0.12, -0.34, 0.56, ..., 0.09]\n",
    "\n",
    "## Vocabulary Size\n",
    "\n",
    "Transformers have a fixed vocabulary size. BERT and DistilBERT have about 30,000 tokens. Multilingual BERT (mBERT) has around 110,000 tokens. The model learns the most common words, prefixes, and suffixes, and unknown words are split into subwords present in the vocabulary.\n",
    "\n",
    "## Python Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f890e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b245c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love mathematics!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc9d4fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'love', 'mathematics', '!']\n",
      "Token IDs: [1045, 2293, 5597, 999]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
