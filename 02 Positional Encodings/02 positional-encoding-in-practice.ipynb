{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba9f7e2",
   "metadata": {},
   "source": [
    "# Positional Encoding Calculations\n",
    "\n",
    "Now that we have our **token embeddings** and simple **position-aware embeddings**, let’s discuss **how the positional encodings are calculated** in practice.\n",
    "\n",
    "## Why use sine and cosine functions?\n",
    "\n",
    "We use the following formulas for the positional encoding $P_{pos, i}$:\n",
    "\n",
    "$$\n",
    "P_{pos, 2i} = \\sin\\Big(\\frac{pos}{10000^{2i/d}}\\Big), \\quad \\text{for even indices} \\\\\n",
    "P_{pos, 2i+1} = \\cos\\Big(\\frac{pos}{10000^{2i/d}}\\Big), \\quad \\text{for odd indices}\n",
    "$$\n",
    "\n",
    "- $pos$ = position of the word in the sequence (starting from 0)\n",
    "- $i$ = dimension index of the embedding\n",
    "- $d$ = total embedding dimension\n",
    "\n",
    "**Reasoning:**\n",
    "\n",
    "1. Each dimension of the positional encoding corresponds to a **different frequency**.\n",
    "2. Sine is used for even indices, cosine for odd indices. This allows the model to **distinguish positions uniquely** and learn relative distances between words.\n",
    "3. The function produces a **continuous and smooth signal** for each position, which the model can exploit for sequence ordering.\n",
    "\n",
    "> This is why the positional encoding vector has the **same size as the embedding vector** $d$ — we need to add them element-wise.\n",
    "\n",
    "## Example with our sentence\n",
    "\n",
    "**Sentence:** `\"The cat is black\"`  \n",
    "**Embedding dimension:** $d = 5$\n",
    "\n",
    "### Step 1: Token embeddings $E_i$\n",
    "\n",
    "| Token | $E_i$                     |\n",
    "| ----- | ------------------------- |\n",
    "| The   | [0.1, 0.2, 0.3, 0.4, 0.5] |\n",
    "| cat   | [0.5, 0.4, 0.3, 0.2, 0.1] |\n",
    "| is    | [0.0, 0.1, 0.0, 0.1, 0.0] |\n",
    "| black | [0.2, 0.2, 0.2, 0.2, 0.2] |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f4be6",
   "metadata": {},
   "source": [
    "### Step 2: Positional encoding $P_i$ (manual small numbers for clarity)\n",
    "\n",
    "Here is calculations for only one token \"The\" at position \"0\" for the sake of brevity.\n",
    "\n",
    "- **Dimension 0** (even) → sine:\n",
    "\n",
    "$$\n",
    "P_{0,0} = \\sin\\Big(\\frac{0}{10000^{0/5}}\\Big) = \\sin(0) = 0\n",
    "$$\n",
    "\n",
    "- **Dimension 1** (odd) → cosine:\n",
    "\n",
    "$$\n",
    "P_{0,1} = \\cos\\Big(\\frac{0}{10000^{0/5}}\\Big) = \\cos(0) = 1\n",
    "$$\n",
    "\n",
    "- **Dimension 2** (even) → sine:\n",
    "\n",
    "$$\n",
    "P_{0,2} = \\sin\\Big(\\frac{0}{10000^{2/5}}\\Big) = \\sin(0) = 0\n",
    "$$\n",
    "\n",
    "- **Dimension 3** (odd) → cosine:\n",
    "\n",
    "$$\n",
    "P_{0,3} = \\cos\\Big(\\frac{0}{10000^{2/5}}\\Big) = \\cos(0) = 1\n",
    "$$\n",
    "\n",
    "- **Dimension 4** (even) → sine:\n",
    "\n",
    "$$\n",
    "P_{0,4} = \\sin\\Big(\\frac{0}{10000^{4/5}}\\Big) = \\sin(0) = 0\n",
    "$$\n",
    "\n",
    "| Position | $P_i$                               |\n",
    "| -------- | ----------------------------------- |\n",
    "| 0        | [0.0, 1.0, 0.0, 1.0, 0.0]           |\n",
    "| 1        | [0.841, 0.540, 0.841, 0.540, 0.841] |\n",
    "| 2        | [0.909, 0.141, 0.909, 0.141, 0.909] |\n",
    "| 3        | [0.141, 0.990, 0.141, 0.990, 0.141] |\n",
    "\n",
    "> Here, even indices (0,2,4) use **sine**, odd indices (1,3) use **cosine**.\n",
    "\n",
    "### Step 3: Position-aware embeddings\n",
    "\n",
    "We add the token embedding and positional encoding element-wise:\n",
    "\n",
    "$$\n",
    "X_i = E_i + P_i\n",
    "$$\n",
    "\n",
    "Calculations for each word:\n",
    "\n",
    "$$\n",
    "X_\\text{The} = [0.1+0.0, 0.2+1.0, 0.3+0.0, 0.4+1.0, 0.5+0.0] = [0.1, 1.2, 0.3, 1.4, 0.5] \\\\\n",
    "X_\\text{cat} = [0.5+0.841, 0.4+0.540, 0.3+0.841, 0.2+0.540, 0.1+0.841] = [1.341, 0.94, 1.141, 0.74, 0.941] \\\\\n",
    "X_\\text{is} = [0.0+0.909, 0.1+0.141, 0.0+0.909, 0.1+0.141, 0.0+0.909] = [0.909, 0.241, 0.909, 0.241, 0.909] \\\\\n",
    "X_\\text{black} = [0.2+0.141, 0.2+0.990, 0.2+0.141, 0.2+0.990, 0.2+0.141] = [0.341, 1.19, 0.341, 1.19, 0.341]\n",
    "$$\n",
    "\n",
    "### Final position-aware embeddings\n",
    "\n",
    "| Token | $X_i$                               |\n",
    "| ----- | ----------------------------------- |\n",
    "| The   | [0.1, 1.2, 0.3, 1.4, 0.5]           |\n",
    "| cat   | [1.341, 0.94, 1.141, 0.74, 0.941]   |\n",
    "| is    | [0.909, 0.241, 0.909, 0.241, 0.909] |\n",
    "| black | [0.341, 1.19, 0.341, 1.19, 0.341]   |\n",
    "\n",
    "> Notice how **each word now encodes its position** in the sentence. These vectors can be fed into the Transformer model to preserve **order information**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa24bd9",
   "metadata": {},
   "source": [
    "### Each word now has a unique fingerprint that combines its meaning and its position because:\n",
    "\n",
    "1. **Sine and cosine create a unique combination**:  \n",
    "   Even if sine or cosine repeats at some positions (e.g., $\\cos(0)=\\cos(2\\pi)=1$), the combination across all dimensions remains unique.\n",
    "\n",
    "2. **High-dimensional embeddings prevent collisions**:  \n",
    "   Transformers use $d \\sim 768$ (BERT/DistilBERT). Even if some waves repeat, the probability that **all 768 values repeat simultaneously** is practically zero.\n",
    "\n",
    "3. **Captures relative positions**:  \n",
    "   Differences between positional encodings of two words give the model **relative distance information**.\n",
    "\n",
    "4. **Scalable for long sequences**:  \n",
    "   Periodicity allows encoding of sentences longer than seen during training without losing uniqueness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f2d08",
   "metadata": {},
   "source": [
    "At this point, we are **done with constructing the positional encodings** and adding them to the token embeddings. Transformers **do not have any built-in notion of order**. Unlike RNNs or CNNs, they process all tokens **in parallel**. By adding $P_i$ to $E_i$, we explicitly inject **word order information** into the embeddings.\n",
    "\n",
    "Each $X_i$ now contains:\n",
    "\n",
    "- semantic information (from $E_i$)\n",
    "- positional information (from $P_i$)\n",
    "\n",
    "This combined representation acts as a **unique fingerprint** for each word **at its specific position** in the sentence.\n",
    "\n",
    "Now we are ready to provide these vectors as the **input embeddings** to the Transformer model.\n",
    "\n",
    "- These $X_i$ vectors are passed directly into the **self-attention layers**\n",
    "- From this point on, the model operates **only on these position-aware embeddings**\n",
    "\n",
    "This is exactly what happens in models like **BERT** and **DistilBERT**, except with much higher dimensionality (e.g., $d=768$).\n",
    "\n",
    "![Positional Encoding](../FIGS/positional-embedding.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
