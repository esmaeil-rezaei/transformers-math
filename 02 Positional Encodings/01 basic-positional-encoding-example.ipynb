{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc4a761",
   "metadata": {},
   "source": [
    "# Positional Encodings in Transformer Architectures\n",
    "\n",
    "Both BERT and DistilBERT share the same overall structure: an embedding layer followed by multiple transformer encoder layers. The key architectural difference is depth—BERT uses more transformer layers than DistilBERT—but the internal mechanics of each layer are the same.\n",
    "\n",
    "Each transformer layer contains multi-head self-attention, normalization steps, and feed-forward networks. However, there is a critical component that sits _between_ the embedding layer and the transformer layers: **positional encodings**.\n",
    "\n",
    "## Why Positional Encodings Are Needed\n",
    "\n",
    "Transformers process input tokens **in parallel**, not sequentially. Unlike recurrent models, they do not read a sentence from left to right or right to left. This parallelism is what makes transformers efficient, but it introduces a problem: **word order is not inherently known to the model**.\n",
    "\n",
    "Without additional information, a transformer cannot distinguish between:\n",
    "\n",
    "`\"The cat sat on the mat\"`\n",
    "\n",
    "and\n",
    "\n",
    "`\"On the mat sat the cat\"`\n",
    "\n",
    "Both sentences contain the same tokens, but their meanings differ because of word order.\n",
    "\n",
    "Positional encodings solve this problem by injecting information about **token position** into the model.\n",
    "\n",
    "## Comparison with Sequential Models\n",
    "\n",
    "In traditional RNN-based encoders, tokens are processed one at a time:\n",
    "\n",
    "Input sequence:  \n",
    "\"the cat is black\"\n",
    "\n",
    "Processing order:\n",
    "\n",
    "- \"the\" → first\n",
    "- \"cat\" → second\n",
    "- \"is\" → third\n",
    "- \"black\" → fourth\n",
    "\n",
    "Because the sequence is processed step by step, the model naturally learns word order.\n",
    "\n",
    "In contrast, a transformer encoder receives all tokens at once:\n",
    "\n",
    "Input tokens:\n",
    "[\"the\", \"cat\", \"is\", \"black\"]\n",
    "\n",
    "All tokens are processed simultaneously. Without positional encodings, the model has no way to know which word came first or last.\n",
    "\n",
    "![RNN vs. Transformer encoding](../FIGs/rnn-transformer.png)\n",
    "\n",
    "## How Positional Encodings Work\n",
    "\n",
    "After tokenization, each token is converted into an embedding vector. Positional encodings are **added directly to these embeddings** before they are passed into the transformer layers.\n",
    "\n",
    "Conceptually:\n",
    "\n",
    "$$\\text{Position-aware embeddings}=\\text{Token embeddings}+\\text{Positional encodings}$$\n",
    "\n",
    "Each position in the sentence is associated with a unique vector that represents its position. These vectors encode either absolute or relative position information, allowing the model to learn patterns such as word order, distance, and structure.\n",
    "\n",
    "At this stage, it is not necessary to focus on the exact numerical form of these encodings. What matters is their role: **they give the model access to word order information**.\n",
    "\n",
    "![Positional Encoding](../FIGS/positional-embedding.png)\n",
    "\n",
    "## Position-Aware Embeddings Example\n",
    "\n",
    "**Sentence:** \"The cat is black\"  \n",
    "**Embedding dimension:** 5\n",
    "\n",
    "### 1. Token Embeddings\n",
    "\n",
    "| Token | Embedding $E_i$           |\n",
    "| ----- | ------------------------- |\n",
    "| The   | [0.1, 0.2, 0.3, 0.4, 0.5] |\n",
    "| cat   | [0.5, 0.4, 0.3, 0.2, 0.1] |\n",
    "| is    | [0.0, 0.1, 0.0, 0.1, 0.0] |\n",
    "| black | [0.2, 0.2, 0.2, 0.2, 0.2] |\n",
    "\n",
    "### 2. Positional Encodings\n",
    "\n",
    "| Position | Positional Encoding $P_i$      |\n",
    "| -------- | ------------------------------ |\n",
    "| 1        | [0.01, 0.02, 0.03, 0.04, 0.05] |\n",
    "| 2        | [0.02, 0.01, 0.02, 0.01, 0.02] |\n",
    "| 3        | [0.03, 0.03, 0.03, 0.03, 0.03] |\n",
    "| 4        | [0.04, 0.02, 0.01, 0.03, 0.05] |\n",
    "\n",
    "### 3. Position-Aware Embeddings\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "X_i = E_i + P_i\n",
    "$$\n",
    "\n",
    "**Calculations:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X_\\text{The} &= [0.1+0.01, 0.2+0.02, 0.3+0.03, 0.4+0.04, 0.5+0.05] = [0.11, 0.22, 0.33, 0.44, 0.55] \\\\\n",
    "X_\\text{cat} &= [0.5+0.02, 0.4+0.01, 0.3+0.02, 0.2+0.01, 0.1+0.02] = [0.52, 0.41, 0.32, 0.21, 0.12] \\\\\n",
    "X_\\text{is} &= [0.0+0.03, 0.1+0.03, 0.0+0.03, 0.1+0.03, 0.0+0.03] = [0.03, 0.13, 0.03, 0.13, 0.03] \\\\\n",
    "X_\\text{black} &= [0.2+0.04, 0.2+0.02, 0.2+0.01, 0.2+0.03, 0.2+0.05] = [0.24, 0.22, 0.21, 0.23, 0.25]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Resulting Position-Aware Embeddings\n",
    "\n",
    "| Token | $X_i$                          |\n",
    "| ----- | ------------------------------ |\n",
    "| The   | [0.11, 0.22, 0.33, 0.44, 0.55] |\n",
    "| cat   | [0.52, 0.41, 0.32, 0.21, 0.12] |\n",
    "| is    | [0.03, 0.13, 0.03, 0.13, 0.03] |\n",
    "| black | [0.24, 0.22, 0.21, 0.23, 0.25] |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
